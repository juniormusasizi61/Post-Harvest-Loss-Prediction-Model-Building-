{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juniormusasizi61/Post-Harvest-Loss-Prediction-Model-Building-/blob/main/deep_q_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGShux4kvOaj",
        "outputId": "784636b4-5b26-4913-fe08-2c5e4943ed90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpxk3_eYbomp"
      },
      "source": [
        "**Data Preparation & Feature Selection**\n",
        "\n",
        "\n",
        "(This stage is shared across several models as they benefit from having only the most relevant inputs.)\n",
        "\n",
        "Before diving into the model implementations, we must prepare our data. We first clean the data, encode categorical variables, and normalize numeric ones. Then we use Lasso (L1 regularization) to select a sparse set of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKS0U8bVHpD",
        "outputId": "bdfd597e-115e-437b-e6b0-375619264cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'm49_code', 'cpc_code', 'commodity', 'year', 'loss_percentage_original', 'loss_quantity', 'food_supply_stage', 'treatment', 'cause_of_loss', 'sample_size', 'method_data_collection', 'reference', 'url', 'notes', 'loss_percentage_0.00596777', 'loss_percentage_0.01', 'loss_percentage_0.0124215', 'loss_percentage_0.0138344', 'loss_percentage_0.02', 'loss_percentage_0.0232144', 'loss_percentage_0.0291458', 'loss_percentage_0.03', 'loss_percentage_0.0302437', 'loss_percentage_0.0336077', 'loss_percentage_0.033767', 'loss_percentage_0.0347729', 'loss_percentage_0.04', 'loss_percentage_0.05', 'loss_percentage_0.06', 'loss_percentage_0.07', 'loss_percentage_0.08', 'loss_percentage_0.0806246', 'loss_percentage_0.0819526', 'loss_percentage_0.0820745', 'loss_percentage_0.083543', 'loss_percentage_0.0847629', 'loss_percentage_0.0876717', 'loss_percentage_0.089166', 'loss_percentage_0.09', 'loss_percentage_0.0933903', 'loss_percentage_0.0958458', 'loss_percentage_0.0972093', 'loss_percentage_0.0990785', 'loss_percentage_0.0997414', 'loss_percentage_0.1', 'loss_percentage_0.107458', 'loss_percentage_0.107738', 'loss_percentage_0.11', 'loss_percentage_0.111384', 'loss_percentage_0.111663', 'loss_percentage_0.117355', 'loss_percentage_0.117518', 'loss_percentage_0.11768', 'loss_percentage_0.12', 'loss_percentage_0.12513', 'loss_percentage_0.125746', 'loss_percentage_0.1262', 'loss_percentage_0.13', 'loss_percentage_0.130329', 'loss_percentage_0.13948', 'loss_percentage_0.14', 'loss_percentage_0.140241', 'loss_percentage_0.142395', 'loss_percentage_0.143871', 'loss_percentage_0.15', 'loss_percentage_0.150937', 'loss_percentage_0.16', 'loss_percentage_0.160268', 'loss_percentage_0.163365', 'loss_percentage_0.16892', 'loss_percentage_0.169796', 'loss_percentage_0.17', 'loss_percentage_0.170146', 'loss_percentage_0.173137', 'loss_percentage_0.17372', 'loss_percentage_0.175956', 'loss_percentage_0.176535', 'loss_percentage_0.18', 'loss_percentage_0.180491', 'loss_percentage_0.183142', 'loss_percentage_0.19', 'loss_percentage_0.192264', 'loss_percentage_0.199877', 'loss_percentage_0.2', 'loss_percentage_0.207432', 'loss_percentage_0.208224', 'loss_percentage_0.21', 'loss_percentage_0.21152', 'loss_percentage_0.211857', 'loss_percentage_0.212691', 'loss_percentage_0.214442', 'loss_percentage_0.22', 'loss_percentage_0.221032', 'loss_percentage_0.23', 'loss_percentage_0.233434', 'loss_percentage_0.234607', 'loss_percentage_0.24', 'loss_percentage_0.241682', 'loss_percentage_0.242173', 'loss_percentage_0.244579', 'loss_percentage_0.25', 'loss_percentage_0.250908', 'loss_percentage_0.253133', 'loss_percentage_0.26', 'loss_percentage_0.260431', 'loss_percentage_0.261696', 'loss_percentage_0.266621', 'loss_percentage_0.268543', 'loss_percentage_0.27', 'loss_percentage_0.271112', 'loss_percentage_0.27144', 'loss_percentage_0.276575', 'loss_percentage_0.277', 'loss_percentage_0.279029', 'loss_percentage_0.28', 'loss_percentage_0.289609', 'loss_percentage_0.29', 'loss_percentage_0.290058', 'loss_percentage_0.293788', 'loss_percentage_0.29792', 'loss_percentage_0.298457', 'loss_percentage_0.298743', 'loss_percentage_0.298948', 'loss_percentage_0.299106', 'loss_percentage_0.3', 'loss_percentage_0.300305', 'loss_percentage_0.301389', 'loss_percentage_0.301421', 'loss_percentage_0.301637', 'loss_percentage_0.303185', 'loss_percentage_0.303569', 'loss_percentage_0.303976', 'loss_percentage_0.304485', 'loss_percentage_0.304901', 'loss_percentage_0.306794', 'loss_percentage_0.307413', 'loss_percentage_0.308276', 'loss_percentage_0.31', 'loss_percentage_0.310036', 'loss_percentage_0.31314', 'loss_percentage_0.313316', 'loss_percentage_0.317203', 'loss_percentage_0.318208', 'loss_percentage_0.32', 'loss_percentage_0.320416', 'loss_percentage_0.322376', 'loss_percentage_0.322646', 'loss_percentage_0.326844', 'loss_percentage_0.327472', 'loss_percentage_0.327877', 'loss_percentage_0.329618', 'loss_percentage_0.33', 'loss_percentage_0.330557', 'loss_percentage_0.330634', 'loss_percentage_0.335', 'loss_percentage_0.33548', 'loss_percentage_0.3355', 'loss_percentage_0.34', 'loss_percentage_0.340611', 'loss_percentage_0.341522', 'loss_percentage_0.341523', 'loss_percentage_0.341553', 'loss_percentage_0.343296', 'loss_percentage_0.346873', 'loss_percentage_0.34713', 'loss_percentage_0.347131', 'loss_percentage_0.348301', 'loss_percentage_0.35', 'loss_percentage_0.351319', 'loss_percentage_0.36', 'loss_percentage_0.360946', 'loss_percentage_0.361342', 'loss_percentage_0.362507', 'loss_percentage_0.37', 'loss_percentage_0.373712', 'loss_percentage_0.373851', 'loss_percentage_0.376607', 'loss_percentage_0.38', 'loss_percentage_0.380967', 'loss_percentage_0.381738', 'loss_percentage_0.383518', 'loss_percentage_0.384637', 'loss_percentage_0.385778', 'loss_percentage_0.389441', 'loss_percentage_0.39', 'loss_percentage_0.391577', 'loss_percentage_0.392019', 'loss_percentage_0.393194', 'loss_percentage_0.393566', 'loss_percentage_0.395', 'loss_percentage_0.397314', 'loss_percentage_0.397945', 'loss_percentage_0.397996', 'loss_percentage_0.39848', 'loss_percentage_0.398522', 'loss_percentage_0.399599', 'loss_percentage_0.4', 'loss_percentage_0.403053', 'loss_percentage_0.404665', 'loss_percentage_0.404847', 'loss_percentage_0.40495', 'loss_percentage_0.405337', 'loss_percentage_0.40605', 'loss_percentage_0.406572', 'loss_percentage_0.411918', 'loss_percentage_0.411998', 'loss_percentage_0.412482', 'loss_percentage_0.413412', 'loss_percentage_0.414342', 'loss_percentage_0.417007', 'loss_percentage_0.419263', 'loss_percentage_0.42', 'loss_percentage_0.420922', 'loss_percentage_0.422546', 'loss_percentage_0.422735', 'loss_percentage_0.423966', 'loss_percentage_0.43', 'loss_percentage_0.433446', 'loss_percentage_0.434283', 'loss_percentage_0.441217', 'loss_percentage_0.44503', 'loss_percentage_0.445628', 'loss_percentage_0.448564', 'loss_percentage_0.45', 'loss_percentage_0.450808', 'loss_percentage_0.455776', 'loss_percentage_0.456127', 'loss_percentage_0.458478', 'loss_percentage_0.460501', 'loss_percentage_0.462663', 'loss_percentage_0.465042', 'loss_percentage_0.466407', 'loss_percentage_0.468809', 'loss_percentage_0.469201', 'loss_percentage_0.46994', 'loss_percentage_0.47', 'loss_percentage_0.470369', 'loss_percentage_0.471596', 'loss_percentage_0.472236', 'loss_percentage_0.473473', 'loss_percentage_0.474132', 'loss_percentage_0.476495', 'loss_percentage_0.477064', 'loss_percentage_0.478263', 'loss_percentage_0.48', 'loss_percentage_0.480229', 'loss_percentage_0.480964', 'loss_percentage_0.481093', 'loss_percentage_0.481848', 'loss_percentage_0.482877', 'loss_percentage_0.483411', 'loss_percentage_0.483552', 'loss_percentage_0.484301', 'loss_percentage_0.484994', 'loss_percentage_0.48569', 'loss_percentage_0.487701', 'loss_percentage_0.488155', 'loss_percentage_0.48966', 'loss_percentage_0.489729', 'loss_percentage_0.49', 'loss_percentage_0.490153', 'loss_percentage_0.490286', 'loss_percentage_0.490735', 'loss_percentage_0.490769', 'loss_percentage_0.490924', 'loss_percentage_0.491015', 'loss_percentage_0.49187', 'loss_percentage_0.492524', 'loss_percentage_0.492531', 'loss_percentage_0.492985', 'loss_percentage_0.494536', 'loss_percentage_0.499467', 'loss_percentage_0.5', 'loss_percentage_0.502376', 'loss_percentage_0.510036', 'loss_percentage_0.511016', 'loss_percentage_0.511668', 'loss_percentage_0.512526', 'loss_percentage_0.513049', 'loss_percentage_0.514433', 'loss_percentage_0.514583', 'loss_percentage_0.515', 'loss_percentage_0.519396', 'loss_percentage_0.52', 'loss_percentage_0.520734', 'loss_percentage_0.524963', 'loss_percentage_0.525323', 'loss_percentage_0.529845', 'loss_percentage_0.53', 'loss_percentage_0.532424', 'loss_percentage_0.533387', 'loss_percentage_0.533913', 'loss_percentage_0.534006', 'loss_percentage_0.534799', 'loss_percentage_0.54', 'loss_percentage_0.541129', 'loss_percentage_0.542403', 'loss_percentage_0.542519', 'loss_percentage_0.548417', 'loss_percentage_0.55', 'loss_percentage_0.553484', 'loss_percentage_0.554189', 'loss_percentage_0.555508', 'loss_percentage_0.556225', 'loss_percentage_0.558542', 'loss_percentage_0.558939', 'loss_percentage_0.56', 'loss_percentage_0.560229', 'loss_percentage_0.560876', 'loss_percentage_0.562125', 'loss_percentage_0.563577', 'loss_percentage_0.563653', 'loss_percentage_0.565351', 'loss_percentage_0.566827', 'loss_percentage_0.568526', 'loss_percentage_0.569206', 'loss_percentage_0.569804', 'loss_percentage_0.570163', 'loss_percentage_0.570526', 'loss_percentage_0.572853', 'loss_percentage_0.573623', 'loss_percentage_0.575324', 'loss_percentage_0.576642', 'loss_percentage_0.576823', 'loss_percentage_0.578771', 'loss_percentage_0.579349', 'loss_percentage_0.58', 'loss_percentage_0.580688', 'loss_percentage_0.581976', 'loss_percentage_0.582589', 'loss_percentage_0.582789', 'loss_percentage_0.584632', 'loss_percentage_0.585512', 'loss_percentage_0.586073', 'loss_percentage_0.586563', 'loss_percentage_0.587405', 'loss_percentage_0.5875', 'loss_percentage_0.59', 'loss_percentage_0.590399', 'loss_percentage_0.591238', 'loss_percentage_0.592334', 'loss_percentage_0.593049', 'loss_percentage_0.593118', 'loss_percentage_0.595267', 'loss_percentage_0.596593', 'loss_percentage_0.599998', 'loss_percentage_0.6', 'loss_percentage_0.600153', 'loss_percentage_0.600908', 'loss_percentage_0.601152', 'loss_percentage_0.601738', 'loss_percentage_0.601824', 'loss_percentage_0.602813', 'loss_percentage_0.603339', 'loss_percentage_0.603807', 'loss_percentage_0.604676', 'loss_percentage_0.606003', 'loss_percentage_0.606046', 'loss_percentage_0.608367', 'loss_percentage_0.610124', 'loss_percentage_0.612202', 'loss_percentage_0.612411', 'loss_percentage_0.614511', 'loss_percentage_0.614965', 'loss_percentage_0.614996', 'loss_percentage_0.617057', 'loss_percentage_0.618128', 'loss_percentage_0.618658', 'loss_percentage_0.62', 'loss_percentage_0.620035', 'loss_percentage_0.620511', 'loss_percentage_0.621083', 'loss_percentage_0.623672', 'loss_percentage_0.62533', 'loss_percentage_0.63', 'loss_percentage_0.632091', 'loss_percentage_0.634118', 'loss_percentage_0.635192', 'loss_percentage_0.636156', 'loss_percentage_0.637125', 'loss_percentage_0.64', 'loss_percentage_0.645551', 'loss_percentage_0.65', 'loss_percentage_0.653647', 'loss_percentage_0.654049', 'loss_percentage_0.657294', 'loss_percentage_0.663096', 'loss_percentage_0.663326', 'loss_percentage_0.665205', 'loss_percentage_0.666008', 'loss_percentage_0.668538', 'loss_percentage_0.67', 'loss_percentage_0.670105', 'loss_percentage_0.672499', 'loss_percentage_0.676122', 'loss_percentage_0.677215', 'loss_percentage_0.679853', 'loss_percentage_0.680619', 'loss_percentage_0.684375', 'loss_percentage_0.685034', 'loss_percentage_0.685548', 'loss_percentage_0.685584', 'loss_percentage_0.685856', 'loss_percentage_0.687735', 'loss_percentage_0.69', 'loss_percentage_0.690699', 'loss_percentage_0.691178', 'loss_percentage_0.693918', 'loss_percentage_0.694258', 'loss_percentage_0.699168', 'loss_percentage_0.699557', 'loss_percentage_0.699662', 'loss_percentage_0.7', 'loss_percentage_0.700634', 'loss_percentage_0.70162', 'loss_percentage_0.702758', 'loss_percentage_0.70291', 'loss_percentage_0.703226', 'loss_percentage_0.703651', 'loss_percentage_0.704395', 'loss_percentage_0.705657', 'loss_percentage_0.707161', 'loss_percentage_0.707565', 'loss_percentage_0.708524', 'loss_percentage_0.708707', 'loss_percentage_0.709391', 'loss_percentage_0.709428', 'loss_percentage_0.709608', 'loss_percentage_0.71', 'loss_percentage_0.710066', 'loss_percentage_0.718601', 'loss_percentage_0.719158', 'loss_percentage_0.72', 'loss_percentage_0.722421', 'loss_percentage_0.723976', 'loss_percentage_0.72427', 'loss_percentage_0.725667', 'loss_percentage_0.72601', 'loss_percentage_0.728308', 'loss_percentage_0.729799', 'loss_percentage_0.73', 'loss_percentage_0.730618', 'loss_percentage_0.734483', 'loss_percentage_0.736687', 'loss_percentage_0.737068', 'loss_percentage_0.737475', 'loss_percentage_0.737625', 'loss_percentage_0.739553', 'loss_percentage_0.74', 'loss_percentage_0.741834', 'loss_percentage_0.742476', 'loss_percentage_0.745252', 'loss_percentage_0.749752', 'loss_percentage_0.75', 'loss_percentage_0.751871', 'loss_percentage_0.752313', 'loss_percentage_0.753828', 'loss_percentage_0.760514', 'loss_percentage_0.764556', 'loss_percentage_0.765194', 'loss_percentage_0.768756', 'loss_percentage_0.769883', 'loss_percentage_0.77', 'loss_percentage_0.773008', 'loss_percentage_0.776372', 'loss_percentage_0.779975', 'loss_percentage_0.78', 'loss_percentage_0.782452', 'loss_percentage_0.783935', 'loss_percentage_0.784552', 'loss_percentage_0.785492', 'loss_percentage_0.786128', 'loss_percentage_0.788314', 'loss_percentage_0.788479', 'loss_percentage_0.79', 'loss_percentage_0.791906', 'loss_percentage_0.793266', 'loss_percentage_0.793649', 'loss_percentage_0.793743', 'loss_percentage_0.796135', 'loss_percentage_0.796384', 'loss_percentage_0.79713', 'loss_percentage_0.797984', 'loss_percentage_0.798164', 'loss_percentage_0.8', 'loss_percentage_0.80156', 'loss_percentage_0.802777', 'loss_percentage_0.803498', 'loss_percentage_0.806411', 'loss_percentage_0.806941', 'loss_percentage_0.809518', 'loss_percentage_0.81', 'loss_percentage_0.819475', 'loss_percentage_0.819931', 'loss_percentage_0.822009', 'loss_percentage_0.826135', 'loss_percentage_0.827916', 'loss_percentage_0.82861', 'loss_percentage_0.829325', 'loss_percentage_0.83', 'loss_percentage_0.84', 'loss_percentage_0.840041', 'loss_percentage_0.843522', 'loss_percentage_0.844108', 'loss_percentage_0.848453', 'loss_percentage_0.851044', 'loss_percentage_0.851228', 'loss_percentage_0.85137', 'loss_percentage_0.853027', 'loss_percentage_0.855431', 'loss_percentage_0.858289', 'loss_percentage_0.858764', 'loss_percentage_0.859124', 'loss_percentage_0.859764', 'loss_percentage_0.86', 'loss_percentage_0.864993', 'loss_percentage_0.869876', 'loss_percentage_0.87', 'loss_percentage_0.873025', 'loss_percentage_0.875', 'loss_percentage_0.878039', 'loss_percentage_0.878842', 'loss_percentage_0.88', 'loss_percentage_0.88005', 'loss_percentage_0.88099', 'loss_percentage_0.881783', 'loss_percentage_0.88209', 'loss_percentage_0.883034', 'loss_percentage_0.886819', 'loss_percentage_0.89', 'loss_percentage_0.891783', 'loss_percentage_0.891809', 'loss_percentage_0.894689', 'loss_percentage_0.895813', 'loss_percentage_0.89617', 'loss_percentage_0.896627', 'loss_percentage_0.9', 'loss_percentage_0.902403', 'loss_percentage_0.903186', 'loss_percentage_0.905742', 'loss_percentage_0.907078', 'loss_percentage_0.91', 'loss_percentage_0.912722', 'loss_percentage_0.913141', 'loss_percentage_0.913427', 'loss_percentage_0.918792', 'loss_percentage_0.91921', 'loss_percentage_0.92', 'loss_percentage_0.922227', 'loss_percentage_0.92501', 'loss_percentage_0.925671', 'loss_percentage_0.925675', 'loss_percentage_0.926264', 'loss_percentage_0.93', 'loss_percentage_0.930461', 'loss_percentage_0.930734', 'loss_percentage_0.931035', 'loss_percentage_0.931262', 'loss_percentage_0.937606', 'loss_percentage_0.938546', 'loss_percentage_0.939808', 'loss_percentage_0.944989', 'loss_percentage_0.950281', 'loss_percentage_0.956749', 'loss_percentage_0.956864', 'loss_percentage_0.96', 'loss_percentage_0.962859', 'loss_percentage_0.96788', 'loss_percentage_0.972625', 'loss_percentage_0.976493', 'loss_percentage_0.977795', 'loss_percentage_0.979909', 'loss_percentage_0.979931', 'loss_percentage_0.980316', 'loss_percentage_0.988175', 'loss_percentage_0.989391', 'loss_percentage_0.99', 'loss_percentage_0.991599', 'loss_percentage_0.992178', 'loss_percentage_0.996783', 'loss_percentage_1.0', 'loss_percentage_1.00305', 'loss_percentage_1.00693', 'loss_percentage_1.0106', 'loss_percentage_1.01322', 'loss_percentage_1.01838', 'loss_percentage_1.02', 'loss_percentage_1.02351', 'loss_percentage_1.02642', 'loss_percentage_1.02652', 'loss_percentage_1.02721', 'loss_percentage_1.02925', 'loss_percentage_1.03', 'loss_percentage_1.03022', 'loss_percentage_1.03274', 'loss_percentage_1.03488', 'loss_percentage_1.03721', 'loss_percentage_1.03978', 'loss_percentage_1.04011', 'loss_percentage_1.04187', 'loss_percentage_1.04268', 'loss_percentage_1.04559', 'loss_percentage_1.04747', 'loss_percentage_1.05', 'loss_percentage_1.05053', 'loss_percentage_1.05133', 'loss_percentage_1.05161', 'loss_percentage_1.05296', 'loss_percentage_1.05833', 'loss_percentage_1.05834', 'loss_percentage_1.06', 'loss_percentage_1.06155', 'loss_percentage_1.06185', 'loss_percentage_1.06326', 'loss_percentage_1.06707', 'loss_percentage_1.06731', 'loss_percentage_1.06733', 'loss_percentage_1.06736', 'loss_percentage_1.06936', 'loss_percentage_1.07', 'loss_percentage_1.07191', 'loss_percentage_1.073', 'loss_percentage_1.07331', 'loss_percentage_1.07359', 'loss_percentage_1.07475', 'loss_percentage_1.07522', 'loss_percentage_1.07566', 'loss_percentage_1.07875', 'loss_percentage_1.08', 'loss_percentage_1.08054', 'loss_percentage_1.08145', 'loss_percentage_1.08168', 'loss_percentage_1.08344', 'loss_percentage_1.08379', 'loss_percentage_1.08396', 'loss_percentage_1.08507', 'loss_percentage_1.08723', 'loss_percentage_1.09', 'loss_percentage_1.09652', 'loss_percentage_1.09745', 'loss_percentage_1.1', 'loss_percentage_1.1021', 'loss_percentage_1.10317', 'loss_percentage_1.10352', 'loss_percentage_1.10372', 'loss_percentage_1.10642', 'loss_percentage_1.11', 'loss_percentage_1.11057', 'loss_percentage_1.11157', 'loss_percentage_1.1137', 'loss_percentage_1.11543', 'loss_percentage_1.1161', 'loss_percentage_1.11784', 'loss_percentage_1.11909', 'loss_percentage_1.11992', 'loss_percentage_1.12', 'loss_percentage_1.12132', 'loss_percentage_1.12781', 'loss_percentage_1.13', 'loss_percentage_1.13333', 'loss_percentage_1.13695', 'loss_percentage_1.13706', 'loss_percentage_1.13872', 'loss_percentage_1.13971', 'loss_percentage_1.14', 'loss_percentage_1.14017', 'loss_percentage_1.1404', 'loss_percentage_1.14706', 'loss_percentage_1.14948', 'loss_percentage_1.15', 'loss_percentage_1.15055', 'loss_percentage_1.15228', 'loss_percentage_1.15325', 'loss_percentage_1.15429', 'loss_percentage_1.15619', 'loss_percentage_1.1569', 'loss_percentage_1.15766', 'loss_percentage_1.15792', 'loss_percentage_1.15934', 'loss_percentage_1.15982', 'loss_percentage_1.16', 'loss_percentage_1.16027', 'loss_percentage_1.16135', 'loss_percentage_1.16145', 'loss_percentage_1.16261', 'loss_percentage_1.16309', 'loss_percentage_1.16535', 'loss_percentage_1.16791', 'loss_percentage_1.16796', 'loss_percentage_1.17', 'loss_percentage_1.1711', 'loss_percentage_1.17237', 'loss_percentage_1.17252', 'loss_percentage_1.175', 'loss_percentage_1.18', 'loss_percentage_1.18477', 'loss_percentage_1.1881', 'loss_percentage_1.19747', 'loss_percentage_1.19761', 'loss_percentage_1.19792', 'loss_percentage_1.198', 'loss_percentage_1.19809', 'loss_percentage_1.19819', 'loss_percentage_1.19824', 'loss_percentage_1.2', 'loss_percentage_1.20109', 'loss_percentage_1.2065', 'loss_percentage_1.21', 'loss_percentage_1.22', 'loss_percentage_1.24', 'loss_percentage_1.24067', 'loss_percentage_1.24726', 'loss_percentage_1.25', 'loss_percentage_1.25919', 'loss_percentage_1.26', 'loss_percentage_1.26062', 'loss_percentage_1.26194', 'loss_percentage_1.27', 'loss_percentage_1.27216', 'loss_percentage_1.27433', 'loss_percentage_1.27523', 'loss_percentage_1.28', 'loss_percentage_1.28122', 'loss_percentage_1.28252', 'loss_percentage_1.28278', 'loss_percentage_1.28322', 'loss_percentage_1.28333', 'loss_percentage_1.28378', 'loss_percentage_1.28429', 'loss_percentage_1.28454', 'loss_percentage_1.28532', 'loss_percentage_1.28543', 'loss_percentage_1.28599', 'loss_percentage_1.28729', 'loss_percentage_1.2891', 'loss_percentage_1.28979', 'loss_percentage_1.29058', 'loss_percentage_1.29217', 'loss_percentage_1.29302', 'loss_percentage_1.3', 'loss_percentage_1.31', 'loss_percentage_1.31595', 'loss_percentage_1.31663', 'loss_percentage_1.33', 'loss_percentage_1.34', 'loss_percentage_1.35', 'loss_percentage_1.36', 'loss_percentage_1.39467', 'loss_percentage_1.4', 'loss_percentage_1.42661', 'loss_percentage_1.43', 'loss_percentage_1.43067', 'loss_percentage_1.43415', 'loss_percentage_1.44', 'loss_percentage_1.445', 'loss_percentage_1.45', 'loss_percentage_1.45186', 'loss_percentage_1.45958', 'loss_percentage_1.47', 'loss_percentage_1.475', 'loss_percentage_1.49774', 'loss_percentage_1.5', 'loss_percentage_1.50269', 'loss_percentage_1.50847', 'loss_percentage_1.51', 'loss_percentage_1.51943', 'loss_percentage_1.52146', 'loss_percentage_1.52387', 'loss_percentage_1.53', 'loss_percentage_1.53114', 'loss_percentage_1.53912', 'loss_percentage_1.54', 'loss_percentage_1.55', 'loss_percentage_1.55032', 'loss_percentage_1.5506', 'loss_percentage_1.55061', 'loss_percentage_1.55335', 'loss_percentage_1.55732', 'loss_percentage_1.56142', 'loss_percentage_1.56694', 'loss_percentage_1.57', 'loss_percentage_1.57768', 'loss_percentage_1.58', 'loss_percentage_1.58097', 'loss_percentage_1.59', 'loss_percentage_1.59777', 'loss_percentage_1.6', 'loss_percentage_1.61', 'loss_percentage_1.61348', 'loss_percentage_1.62', 'loss_percentage_1.625', 'loss_percentage_1.62554', 'loss_percentage_1.63', 'loss_percentage_1.63471', 'loss_percentage_1.63808', 'loss_percentage_1.64', 'loss_percentage_1.64413', 'loss_percentage_1.64609', 'loss_percentage_1.64639', 'loss_percentage_1.64812', 'loss_percentage_1.66', 'loss_percentage_1.665', 'loss_percentage_1.67209', 'loss_percentage_1.67603', 'loss_percentage_1.67801', 'loss_percentage_1.68', 'loss_percentage_1.6818', 'loss_percentage_1.6872', 'loss_percentage_1.69', 'loss_percentage_1.69066', 'loss_percentage_1.7', 'loss_percentage_1.70007', 'loss_percentage_1.70942', 'loss_percentage_1.71', 'loss_percentage_1.72', 'loss_percentage_1.73', 'loss_percentage_1.74', 'loss_percentage_1.745', 'loss_percentage_1.7493', 'loss_percentage_1.74954', 'loss_percentage_1.75', 'loss_percentage_1.755', 'loss_percentage_1.76', 'loss_percentage_1.77644', 'loss_percentage_1.77655', 'loss_percentage_1.78', 'loss_percentage_1.79', 'loss_percentage_1.8', 'loss_percentage_1.82', 'loss_percentage_1.83', 'loss_percentage_1.83731', 'loss_percentage_1.84', 'loss_percentage_1.84768', 'loss_percentage_1.85', 'loss_percentage_1.8521', 'loss_percentage_1.86', 'loss_percentage_1.86152', 'loss_percentage_1.867', 'loss_percentage_1.88', 'loss_percentage_1.88242', 'loss_percentage_1.89068', 'loss_percentage_1.91', 'loss_percentage_1.93', 'loss_percentage_1.94', 'loss_percentage_1.95', 'loss_percentage_1.95972', 'loss_percentage_1.98', 'loss_percentage_1.98199', 'loss_percentage_1.9876', 'loss_percentage_1.98771', 'loss_percentage_2.0', 'loss_percentage_2.0001', 'loss_percentage_2.00219', 'loss_percentage_2.00374', 'loss_percentage_2.00466', 'loss_percentage_2.01', 'loss_percentage_2.01046', 'loss_percentage_2.01147', 'loss_percentage_2.01801', 'loss_percentage_2.01844', 'loss_percentage_2.03162', 'loss_percentage_2.03968', 'loss_percentage_2.0405', 'loss_percentage_2.04137', 'loss_percentage_2.0462', 'loss_percentage_2.04846', 'loss_percentage_2.04932', 'loss_percentage_2.0499', 'loss_percentage_2.05', 'loss_percentage_2.05431', 'loss_percentage_2.055', 'loss_percentage_2.05942', 'loss_percentage_2.06', 'loss_percentage_2.06041', 'loss_percentage_2.06893', 'loss_percentage_2.07', 'loss_percentage_2.08', 'loss_percentage_2.08079', 'loss_percentage_2.08203', 'loss_percentage_2.09059', 'loss_percentage_2.09953', 'loss_percentage_2.1', 'loss_percentage_2.10963', 'loss_percentage_2.11', 'loss_percentage_2.12476', 'loss_percentage_2.12632', 'loss_percentage_2.13', 'loss_percentage_2.13139', 'loss_percentage_2.14625', 'loss_percentage_2.15', 'loss_percentage_2.16', 'loss_percentage_2.161', 'loss_percentage_2.16184', 'loss_percentage_2.16825', 'loss_percentage_2.1695', 'loss_percentage_2.16951', 'loss_percentage_2.18', 'loss_percentage_2.182', 'loss_percentage_2.18796', 'loss_percentage_2.19', 'loss_percentage_2.2', 'loss_percentage_2.20516', 'loss_percentage_2.2079', 'loss_percentage_2.21104', 'loss_percentage_2.215', 'loss_percentage_2.22', 'loss_percentage_2.22872', 'loss_percentage_2.23', 'loss_percentage_2.23054', 'loss_percentage_2.24', 'loss_percentage_2.25', 'loss_percentage_2.2517', 'loss_percentage_2.26145', 'loss_percentage_2.26352', 'loss_percentage_2.26987', 'loss_percentage_2.27', 'loss_percentage_2.28', 'loss_percentage_2.28551', 'loss_percentage_2.29', 'loss_percentage_2.29218', 'loss_percentage_2.29899', 'loss_percentage_2.29942', 'loss_percentage_2.3', 'loss_percentage_2.30549', 'loss_percentage_2.30578', 'loss_percentage_2.30924', 'loss_percentage_2.35', 'loss_percentage_2.35331', 'loss_percentage_2.35719', 'loss_percentage_2.3621', 'loss_percentage_2.36586', 'loss_percentage_2.37', 'loss_percentage_2.37073', 'loss_percentage_2.37249', 'loss_percentage_2.37281', 'loss_percentage_2.37387', 'loss_percentage_2.37499', 'loss_percentage_2.38075', 'loss_percentage_2.4', 'loss_percentage_2.40415', 'loss_percentage_2.41761', 'loss_percentage_2.42', 'loss_percentage_2.42317', 'loss_percentage_2.4234', 'loss_percentage_2.42725', 'loss_percentage_2.42754', 'loss_percentage_2.43284', 'loss_percentage_2.43296', 'loss_percentage_2.43455', 'loss_percentage_2.43791', 'loss_percentage_2.44603', 'loss_percentage_2.45', 'loss_percentage_2.46', 'loss_percentage_2.46463', 'loss_percentage_2.47', 'loss_percentage_2.47277', 'loss_percentage_2.47513', 'loss_percentage_2.49', 'loss_percentage_2.49297', 'loss_percentage_2.49334', 'loss_percentage_2.5', 'loss_percentage_2.51605', 'loss_percentage_2.51891', 'loss_percentage_2.53', 'loss_percentage_2.56941', 'loss_percentage_2.57449', 'loss_percentage_2.57634', 'loss_percentage_2.57943', 'loss_percentage_2.58', 'loss_percentage_2.58573', 'loss_percentage_2.5868', 'loss_percentage_2.5935', 'loss_percentage_2.59765', 'loss_percentage_2.59925', 'loss_percentage_2.59999', 'loss_percentage_2.6', 'loss_percentage_2.60066', 'loss_percentage_2.6017', 'loss_percentage_2.61001', 'loss_percentage_2.61505', 'loss_percentage_2.61571', 'loss_percentage_2.61612', 'loss_percentage_2.62261', 'loss_percentage_2.62332', 'loss_percentage_2.62509', 'loss_percentage_2.62653', 'loss_percentage_2.62844', 'loss_percentage_2.63', 'loss_percentage_2.63253', 'loss_percentage_2.63395', 'loss_percentage_2.64887', 'loss_percentage_2.64978', 'loss_percentage_2.64993', 'loss_percentage_2.65', 'loss_percentage_2.65112', 'loss_percentage_2.66', 'loss_percentage_2.69', 'loss_percentage_2.7', 'loss_percentage_2.73', 'loss_percentage_2.74', 'loss_percentage_2.74937', 'loss_percentage_2.75', 'loss_percentage_2.75333', 'loss_percentage_2.75944', 'loss_percentage_2.76', 'loss_percentage_2.76204', 'loss_percentage_2.76244', 'loss_percentage_2.78', 'loss_percentage_2.79', 'loss_percentage_2.8', 'loss_percentage_2.81', 'loss_percentage_2.83387', 'loss_percentage_2.84', 'loss_percentage_2.84331', 'loss_percentage_2.87', 'loss_percentage_2.89134', 'loss_percentage_2.89224', 'loss_percentage_2.9', 'loss_percentage_2.91151', 'loss_percentage_2.91839', 'loss_percentage_2.92', 'loss_percentage_2.9209', 'loss_percentage_2.93051', 'loss_percentage_2.93212', 'loss_percentage_2.93383', 'loss_percentage_2.93669', 'loss_percentage_2.93697', 'loss_percentage_2.9379', 'loss_percentage_2.95173', 'loss_percentage_2.95682', 'loss_percentage_2.96', 'loss_percentage_2.9606', 'loss_percentage_2.96457', 'loss_percentage_2.96915', 'loss_percentage_2.97108', 'loss_percentage_2.98', 'loss_percentage_2.98598', 'loss_percentage_2.98763', 'loss_percentage_3.0', 'loss_percentage_3.00603', 'loss_percentage_3.02', 'loss_percentage_3.03', 'loss_percentage_3.04', 'loss_percentage_3.04432', 'loss_percentage_3.05', 'loss_percentage_3.08097', 'loss_percentage_3.08588', 'loss_percentage_3.1', 'loss_percentage_3.10178', 'loss_percentage_3.10246', 'loss_percentage_3.11', 'loss_percentage_3.1101', 'loss_percentage_3.11127', 'loss_percentage_3.12', 'loss_percentage_3.12042', 'loss_percentage_3.12663', 'loss_percentage_3.13691', 'loss_percentage_3.13708', 'loss_percentage_3.13713', 'loss_percentage_3.13714', 'loss_percentage_3.14', 'loss_percentage_3.16', 'loss_percentage_3.2', 'loss_percentage_3.21371', 'loss_percentage_3.2276', 'loss_percentage_3.24', 'loss_percentage_3.25812', 'loss_percentage_3.27', 'loss_percentage_3.29042', 'loss_percentage_3.3', 'loss_percentage_3.33', 'loss_percentage_3.34732', 'loss_percentage_3.35', 'loss_percentage_3.35905', 'loss_percentage_3.36', 'loss_percentage_3.36571', 'loss_percentage_3.375', 'loss_percentage_3.37671', 'loss_percentage_3.38733', 'loss_percentage_3.4', 'loss_percentage_3.41401', 'loss_percentage_3.41428', 'loss_percentage_3.41637', 'loss_percentage_3.42', 'loss_percentage_3.42482', 'loss_percentage_3.42496', 'loss_percentage_3.42562', 'loss_percentage_3.42665', 'loss_percentage_3.42692', 'loss_percentage_3.42733', 'loss_percentage_3.4297', 'loss_percentage_3.43036', 'loss_percentage_3.43063', 'loss_percentage_3.43193', 'loss_percentage_3.4324', 'loss_percentage_3.43312', 'loss_percentage_3.43528', 'loss_percentage_3.43613', 'loss_percentage_3.43657', 'loss_percentage_3.43662', 'loss_percentage_3.44008', 'loss_percentage_3.44365', 'loss_percentage_3.44499', 'loss_percentage_3.44623', 'loss_percentage_3.44988', 'loss_percentage_3.45', 'loss_percentage_3.45049', 'loss_percentage_3.45582', 'loss_percentage_3.45681', 'loss_percentage_3.45716', 'loss_percentage_3.46', 'loss_percentage_3.46495', 'loss_percentage_3.46667', 'loss_percentage_3.47911', 'loss_percentage_3.48', 'loss_percentage_3.48279', 'loss_percentage_3.48317', 'loss_percentage_3.48424', 'loss_percentage_3.48737', 'loss_percentage_3.48739', 'loss_percentage_3.48797', 'loss_percentage_3.49072', 'loss_percentage_3.4928', 'loss_percentage_3.4938', 'loss_percentage_3.4961', 'loss_percentage_3.49766', 'loss_percentage_3.49979', 'loss_percentage_3.49985', 'loss_percentage_3.49987', 'loss_percentage_3.5', 'loss_percentage_3.51', 'loss_percentage_3.52', 'loss_percentage_3.55', 'loss_percentage_3.56', 'loss_percentage_3.57', 'loss_percentage_3.59', 'loss_percentage_3.6', 'loss_percentage_3.62', 'loss_percentage_3.65', 'loss_percentage_3.68', 'loss_percentage_3.68926', 'loss_percentage_3.7', 'loss_percentage_3.74', 'loss_percentage_3.75', 'loss_percentage_3.76763', 'loss_percentage_3.78', 'loss_percentage_3.795', 'loss_percentage_3.8', 'loss_percentage_3.81', 'loss_percentage_3.85', 'loss_percentage_3.86394', 'loss_percentage_3.86756', 'loss_percentage_3.87', 'loss_percentage_3.9', 'loss_percentage_3.92', 'loss_percentage_3.94', 'loss_percentage_3.95', 'loss_percentage_3.96168', 'loss_percentage_3.98', 'loss_percentage_4.0', 'loss_percentage_4.06', 'loss_percentage_4.065', 'loss_percentage_4.07', 'loss_percentage_4.07917', 'loss_percentage_4.1', 'loss_percentage_4.10219', 'loss_percentage_4.11284', 'loss_percentage_4.12', 'loss_percentage_4.14', 'loss_percentage_4.14215', 'loss_percentage_4.14218', 'loss_percentage_4.16', 'loss_percentage_4.161', 'loss_percentage_4.18', 'loss_percentage_4.195', 'loss_percentage_4.21441', 'loss_percentage_4.23', 'loss_percentage_4.25', 'loss_percentage_4.28', 'loss_percentage_4.295', 'loss_percentage_4.2951', 'loss_percentage_4.3', 'loss_percentage_4.32', 'loss_percentage_4.32341', 'loss_percentage_4.32449', 'loss_percentage_4.33076', 'loss_percentage_4.33326', 'loss_percentage_4.33392', 'loss_percentage_4.37029', 'loss_percentage_4.37596', 'loss_percentage_4.4', 'loss_percentage_4.40228', 'loss_percentage_4.40333', 'loss_percentage_4.40406', 'loss_percentage_4.41124', 'loss_percentage_4.41171', 'loss_percentage_4.41192', 'loss_percentage_4.41722', 'loss_percentage_4.43', 'loss_percentage_4.43333', 'loss_percentage_4.43727', 'loss_percentage_4.45044', 'loss_percentage_4.45338', 'loss_percentage_4.48', 'loss_percentage_4.49569', 'loss_percentage_4.4957', 'loss_percentage_4.49851', 'loss_percentage_4.5', 'loss_percentage_4.51233', 'loss_percentage_4.53', 'loss_percentage_4.54', 'loss_percentage_4.55', 'loss_percentage_4.56632', 'loss_percentage_4.56643', 'loss_percentage_4.57084', 'loss_percentage_4.57721', 'loss_percentage_4.59', 'loss_percentage_4.6', 'loss_percentage_4.60995', 'loss_percentage_4.61312', 'loss_percentage_4.61359', 'loss_percentage_4.62', 'loss_percentage_4.62367', 'loss_percentage_4.64', 'loss_percentage_4.65', 'loss_percentage_4.67', 'loss_percentage_4.67306', 'loss_percentage_4.68', 'loss_percentage_4.69', 'loss_percentage_4.7', 'loss_percentage_4.705', 'loss_percentage_4.715', 'loss_percentage_4.74', 'loss_percentage_4.74105', 'loss_percentage_4.74673', 'loss_percentage_4.74778', 'loss_percentage_4.75', 'loss_percentage_4.76355', 'loss_percentage_4.76565', 'loss_percentage_4.76822', 'loss_percentage_4.7712', 'loss_percentage_4.78204', 'loss_percentage_4.78371', 'loss_percentage_4.7851', 'loss_percentage_4.78844', 'loss_percentage_4.79481', 'loss_percentage_4.79905', 'loss_percentage_4.82', 'loss_percentage_4.83', 'loss_percentage_4.86667', 'loss_percentage_4.9', 'loss_percentage_4.92', 'loss_percentage_4.93', 'loss_percentage_4.95', 'loss_percentage_4.97', 'loss_percentage_5.0', 'loss_percentage_5.08184', 'loss_percentage_5.1', 'loss_percentage_5.14019', 'loss_percentage_5.2', 'loss_percentage_5.23', 'loss_percentage_5.26', 'loss_percentage_5.27223', 'loss_percentage_5.32', 'loss_percentage_5.35', 'loss_percentage_5.37', 'loss_percentage_5.4', 'loss_percentage_5.40334', 'loss_percentage_5.43603', 'loss_percentage_5.47371', 'loss_percentage_5.49937', 'loss_percentage_5.5', 'loss_percentage_5.53', 'loss_percentage_5.54', 'loss_percentage_5.6', 'loss_percentage_5.66566', 'loss_percentage_5.6786', 'loss_percentage_5.69', 'loss_percentage_5.7', 'loss_percentage_5.73', 'loss_percentage_5.75', 'loss_percentage_5.79545', 'loss_percentage_5.8', 'loss_percentage_5.88', 'loss_percentage_5.89', 'loss_percentage_5.9', 'loss_percentage_5.92', 'loss_percentage_5.92899', 'loss_percentage_5.93043', 'loss_percentage_5.95', 'loss_percentage_5.96', 'loss_percentage_6.0', 'loss_percentage_6.09', 'loss_percentage_6.19', 'loss_percentage_6.2', 'loss_percentage_6.22', 'loss_percentage_6.275', 'loss_percentage_6.3', 'loss_percentage_6.35412', 'loss_percentage_6.4', 'loss_percentage_6.41', 'loss_percentage_6.42', 'loss_percentage_6.48', 'loss_percentage_6.49', 'loss_percentage_6.5', 'loss_percentage_6.6', 'loss_percentage_6.64', 'loss_percentage_6.65', 'loss_percentage_6.72', 'loss_percentage_6.75721', 'loss_percentage_6.75744', 'loss_percentage_6.79', 'loss_percentage_6.88', 'loss_percentage_6.9', 'loss_percentage_6.98', 'loss_percentage_6.99', 'loss_percentage_7.0', 'loss_percentage_7.01', 'loss_percentage_7.02', 'loss_percentage_7.02239', 'loss_percentage_7.04', 'loss_percentage_7.07', 'loss_percentage_7.07485', 'loss_percentage_7.12406', 'loss_percentage_7.13171', 'loss_percentage_7.14', 'loss_percentage_7.22', 'loss_percentage_7.25', 'loss_percentage_7.26', 'loss_percentage_7.29', 'loss_percentage_7.3', 'loss_percentage_7.44984', 'loss_percentage_7.49', 'loss_percentage_7.5', 'loss_percentage_7.53623', 'loss_percentage_7.54', 'loss_percentage_7.58882', 'loss_percentage_7.6', 'loss_percentage_7.615', 'loss_percentage_7.67', 'loss_percentage_7.7', 'loss_percentage_7.76', 'loss_percentage_7.77', 'loss_percentage_7.8', 'loss_percentage_7.806', 'loss_percentage_7.89', 'loss_percentage_8.0', 'loss_percentage_8.01', 'loss_percentage_8.03', 'loss_percentage_8.03398', 'loss_percentage_8.06', 'loss_percentage_8.1', 'loss_percentage_8.21543', 'loss_percentage_8.26', 'loss_percentage_8.29', 'loss_percentage_8.29737', 'loss_percentage_8.3', 'loss_percentage_8.30417', 'loss_percentage_8.38', 'loss_percentage_8.4', 'loss_percentage_8.41', 'loss_percentage_8.48', 'loss_percentage_8.5', 'loss_percentage_8.54', 'loss_percentage_8.61', 'loss_percentage_8.7', 'loss_percentage_8.8', 'loss_percentage_8.88458', 'loss_percentage_8.88459', 'loss_percentage_8.9', 'loss_percentage_9.0', 'loss_percentage_9.01', 'loss_percentage_9.05', 'loss_percentage_9.05337', 'loss_percentage_9.1', 'loss_percentage_9.15773', 'loss_percentage_9.16', 'loss_percentage_9.16204', 'loss_percentage_9.395', 'loss_percentage_9.41', 'loss_percentage_9.435', 'loss_percentage_9.86301', 'loss_percentage_9.92', 'loss_percentage_10.0', 'loss_percentage_10.1', 'loss_percentage_10.11', 'loss_percentage_10.17', 'loss_percentage_10.25', 'loss_percentage_10.2917', 'loss_percentage_10.3', 'loss_percentage_10.4365', 'loss_percentage_10.455', 'loss_percentage_10.5', 'loss_percentage_10.6', 'loss_percentage_10.65', 'loss_percentage_10.7', 'loss_percentage_10.71', 'loss_percentage_10.74', 'loss_percentage_10.75', 'loss_percentage_10.76', 'loss_percentage_10.7854', 'loss_percentage_10.79', 'loss_percentage_10.8', 'loss_percentage_10.96', 'loss_percentage_10.97', 'loss_percentage_11.0', 'loss_percentage_11.05', 'loss_percentage_11.1', 'loss_percentage_11.3351', 'loss_percentage_11.5', 'loss_percentage_11.59', 'loss_percentage_11.71', 'loss_percentage_11.77', 'loss_percentage_11.8', 'loss_percentage_11.88', 'loss_percentage_11.96', 'loss_percentage_11.97', 'loss_percentage_12.0', 'loss_percentage_12.17', 'loss_percentage_12.2539', 'loss_percentage_12.3', 'loss_percentage_12.4', 'loss_percentage_12.4245', 'loss_percentage_12.44', 'loss_percentage_12.45', 'loss_percentage_12.4996', 'loss_percentage_12.5', 'loss_percentage_12.7017', 'loss_percentage_12.71', 'loss_percentage_12.8', 'loss_percentage_12.9', 'loss_percentage_12.9604', 'loss_percentage_12.98', 'loss_percentage_13.0', 'loss_percentage_13.0341', 'loss_percentage_13.1', 'loss_percentage_13.1331', 'loss_percentage_13.1434', 'loss_percentage_13.21', 'loss_percentage_13.2179', 'loss_percentage_13.25', 'loss_percentage_13.31', 'loss_percentage_13.33', 'loss_percentage_13.4', 'loss_percentage_13.5', 'loss_percentage_13.62', 'loss_percentage_13.7184', 'loss_percentage_13.7209', 'loss_percentage_13.9', 'loss_percentage_13.99', 'loss_percentage_14.0', 'loss_percentage_14.04', 'loss_percentage_14.277', 'loss_percentage_14.3012', 'loss_percentage_14.3423', 'loss_percentage_14.4', 'loss_percentage_14.5', 'loss_percentage_14.54', 'loss_percentage_14.5927', 'loss_percentage_14.7', 'loss_percentage_14.7025', 'loss_percentage_14.8', 'loss_percentage_14.9', 'loss_percentage_15.0', 'loss_percentage_15.0791', 'loss_percentage_15.11', 'loss_percentage_15.14', 'loss_percentage_15.16', 'loss_percentage_15.3', 'loss_percentage_15.37', 'loss_percentage_15.5', 'loss_percentage_15.56', 'loss_percentage_15.6067', 'loss_percentage_15.7', 'loss_percentage_15.8', 'loss_percentage_15.82', 'loss_percentage_16.0', 'loss_percentage_16.0428', 'loss_percentage_16.12', 'loss_percentage_16.24', 'loss_percentage_16.2873', 'loss_percentage_16.2975', 'loss_percentage_16.3', 'loss_percentage_16.35', 'loss_percentage_16.65', 'loss_percentage_16.7', 'loss_percentage_16.75', 'loss_percentage_16.9', 'loss_percentage_17.0', 'loss_percentage_17.1', 'loss_percentage_17.46', 'loss_percentage_17.5', 'loss_percentage_17.65', 'loss_percentage_17.75', 'loss_percentage_17.78', 'loss_percentage_18.0', 'loss_percentage_18.1', 'loss_percentage_18.3', 'loss_percentage_18.6', 'loss_percentage_19.1', 'loss_percentage_20.0', 'loss_percentage_20.01', 'loss_percentage_20.18', 'loss_percentage_21.0', 'loss_percentage_21.2', 'loss_percentage_21.5', 'loss_percentage_21.7', 'loss_percentage_22.0', 'loss_percentage_22.5', 'loss_percentage_23.0', 'loss_percentage_24.0', 'loss_percentage_24.5', 'loss_percentage_24.6', 'loss_percentage_24.9', 'loss_percentage_25.0', 'loss_percentage_25.1', 'loss_percentage_25.2', 'loss_percentage_26.0', 'loss_percentage_26.2', 'loss_percentage_26.4', 'loss_percentage_26.7', 'loss_percentage_27.0', 'loss_percentage_27.2', 'loss_percentage_27.5', 'loss_percentage_28.0', 'loss_percentage_28.6', 'loss_percentage_28.7', 'loss_percentage_28.86', 'loss_percentage_29.0', 'loss_percentage_29.3', 'loss_percentage_29.5', 'loss_percentage_29.75', 'loss_percentage_30.0', 'loss_percentage_31.0', 'loss_percentage_31.2', 'loss_percentage_32.1', 'loss_percentage_32.855', 'loss_percentage_32.9', 'loss_percentage_33.5', 'loss_percentage_34.95', 'loss_percentage_35.0', 'loss_percentage_35.42', 'loss_percentage_35.5', 'loss_percentage_36.9', 'loss_percentage_37.455', 'loss_percentage_39.55', 'loss_percentage_40.0', 'loss_percentage_40.85', 'loss_percentage_41.18', 'loss_percentage_43.0', 'loss_percentage_46.87', 'loss_percentage_47.0', 'loss_percentage_47.5', 'loss_percentage_50.0', 'loss_percentage_50.5', 'loss_percentage_53.0', 'loss_percentage_60.0', 'loss_percentage_60.7', 'loss_percentage_61.1', 'activity_Bagging, Packaging', 'activity_Bulking, Distribution', 'activity_Bundling', 'activity_Cleaning', 'activity_Cleaning, Collection, Distribution, Drying, Field, Harvesting, Shelling, Storage, Threshing, Transportation', 'activity_Cleaning, Collection, Drying, Harvesting, Packaging, Threshing, Transportation, Winnowing', 'activity_Cleaning, Drying, Field, Harvesting, Ripening, Threshing, Transportation, Winnowing', 'activity_Cleaning, Farm', 'activity_Cleaning, Winnowing', 'activity_Collection', 'activity_Collection, Distribution, Marketing, Wholesale', 'activity_Collection, Drying, Farm, Packaging, Winnowing', 'activity_Collection, Drying, Harvesting, Packaging, Processing, Retailing, Storage, Threshing, Transportation, Wholesale, Winnowing', 'activity_Collection, Drying, Harvesting, Processing, Retailing, Storage, Threshing, Transportation, Wholesale, Winnowing', 'activity_Collection, Farm', 'activity_Collection, Grading, Harvesting, Packaging, Retailing, Storage, Transportation, Wholesale', 'activity_Collection, Harvesting, Packaging, Sorting, Transportation', 'activity_Collection, Harvesting, Threshing, Winnowing', 'activity_Consumption', 'activity_Consumption, Retailing', 'activity_Distribution', 'activity_Distribution, Marketing, Storage', 'activity_Drying', 'activity_Drying, Farm', 'activity_Drying, Harvesting', 'activity_Drying, Storage', 'activity_Drying, Threshing', 'activity_Farm', 'activity_Farm, Handling, Storage', 'activity_Farm, Harvesting', 'activity_Farm, Harvesting, Retailing', 'activity_Farm, Manufacturing, Transportation', 'activity_Farm, Retailing, Trading, Wholesale', 'activity_Farm, Sorting', 'activity_Farm, Storage', 'activity_Farm, Threshing', 'activity_Grading', 'activity_Grading, Packaging', 'activity_Grading, Sorting', 'activity_Grading, Transportation', 'activity_Handling', 'activity_Handling, Storage', 'activity_Handling, Transportation', 'activity_Harvesting', 'activity_Loading', 'activity_Loading, Marketing, Transportation, Unloading', 'activity_Loading, Unloading', 'activity_Marketing', 'activity_Marketing, Retailing, Wholesale', 'activity_Marketing, Storage', 'activity_Marketing, Storage, Transportation', 'activity_Marketing, Transportation', 'activity_Marketing, Wholesale', 'activity_Milling', 'activity_Milling, Processing', 'activity_Milling, Storage', 'activity_Packaging', 'activity_Packaging, Processing', 'activity_Packaging, Transportation', 'activity_Parboiling', 'activity_Processing', 'activity_Retailing', 'activity_Shelling', 'activity_Shelling, Threshing', 'activity_Sorting', 'activity_Stacking', 'activity_Storage', 'activity_Threshing', 'activity_Trading', 'activity_Transportation', 'activity_Wholesale', 'activity_Winnowing']\n",
            "✅ Data Preprocessing Complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Load the Dataset ---\n",
        "# Read the CSV file into a pandas DataFrame.\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/post_harvest.csv\")\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "\n",
        "# Define the numerical columns.\n",
        "numerical_columns = [\"loss_percentage_original\", \"cpc_code\", \"year\"]\n",
        "\n",
        "# Convert the numerical columns to numeric types.\n",
        "# Any values that cannot be converted will be set to NaN.\n",
        "data[numerical_columns] = data[numerical_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Impute missing values in numerical columns with the mean of each column.\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "data[numerical_columns] = num_imputer.fit_transform(data[numerical_columns])\n",
        "\n",
        "# Define the categorical columns.\n",
        "categorical_columns = [\"loss_percentage\", \"activity\"]\n",
        "\n",
        "# Impute missing values in categorical columns with the most frequent value.\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "data[categorical_columns] = cat_imputer.fit_transform(data[categorical_columns])\n",
        "\n",
        "# --- Encode Categorical Variables ---\n",
        "\n",
        "# # Initialize OneHotEncoder:\n",
        "# - sparse_output=False returns a dense array.\n",
        "# - drop=\"first\" avoids the dummy variable trap by dropping the first category.\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
        "encoded_categories = encoder.fit_transform(data[categorical_columns])\n",
        "\n",
        "# Create a DataFrame from the encoded categories.\n",
        "encoded_category_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# Remove the original categorical columns from the dataset.\n",
        "data = data.drop(columns=categorical_columns)\n",
        "\n",
        "# Append the encoded categorical DataFrame to the original DataFrame.\n",
        "data = pd.concat([data, encoded_category_df], axis=1)\n",
        "\n",
        "# Optionally, print the list of columns to verify the changes.\n",
        "print(data.columns.tolist())\n",
        "\n",
        "# --- Normalize Numerical Features ---\n",
        "\n",
        "# Initialize StandardScaler to standardize numerical features.\n",
        "scaler = StandardScaler()\n",
        "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "# --- Save the Preprocessed Dataset ---\n",
        "\n",
        "# Export the processed DataFrame to a new CSV file. which is compressed\n",
        "data.to_csv(\"processed_post_harvest_data.csv\", index=False, compression=\"gzip\")\n",
        "print(\"✅ Data Preprocessing Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvLnB6WyYhrh",
        "outputId": "36715a21-258b-4d34-b815-bf380fa64387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: ['Unnamed: 0', 'm49_code', 'food_supply_stage_Harvest']\n",
            "Number of selected features: 3\n",
            "✅ Feature Selection Complete!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data with appropriate parameters to handle compression\n",
        "data = pd.read_csv(\"/content/processed_post_harvest_data.csv\",\n",
        "                   compression=\"gzip\",\n",
        "                   low_memory=False)\n",
        "\n",
        "# Separate features and target variables\n",
        "# We exclude all columns that represent loss percentages from our features\n",
        "X = data[[col for col in data.columns if not col.startswith('loss_percentage_')]]\n",
        "y = data[[col for col in data.columns if col.startswith('loss_percentage_')]]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# The random_state ensures reproducibility of results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Ensure consistent columns between train and test sets after encoding\n",
        "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded,\n",
        "                                                       join='inner',\n",
        "                                                       axis=1)\n",
        "\n",
        "# Create and fit the feature selector\n",
        "# We increase max_iter to ensure convergence\n",
        "selector = SelectFromModel(estimator=Lasso(alpha=0.1, max_iter=1000))\n",
        "\n",
        "# Since y might have multiple columns (multiple loss percentage targets),\n",
        "# we need to handle this appropriately\n",
        "if y_train.shape[1] > 1:\n",
        "    # If we have multiple targets, let's use the first one for feature selection\n",
        "    # You might want to adjust this strategy based on your specific needs\n",
        "    selector.fit(X_train_encoded, y_train.iloc[:, 0])\n",
        "else:\n",
        "    selector.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Get the selected feature mask and feature names\n",
        "selected_features_mask = selector.get_support()\n",
        "selected_features = X_train_encoded.columns[selected_features_mask].tolist()\n",
        "\n",
        "# Print the selected features\n",
        "print(\"Selected Features:\", selected_features)\n",
        "print(f\"Number of selected features: {len(selected_features)}\")\n",
        "\n",
        "# Select only the important features for both training and test sets\n",
        "X_train_selected = X_train_encoded[selected_features]\n",
        "X_test_selected = X_test_encoded[selected_features]\n",
        "\n",
        "# Save the processed datasets\n",
        "X_train_selected.to_csv(\"X_train_selected.csv\", index=False)\n",
        "X_test_selected.to_csv(\"X_test_selected.csv\", index=False)\n",
        "y_train.to_csv(\"y_train.csv\", index=False)\n",
        "y_test.to_csv(\"y_test.csv\", index=False)\n",
        "\n",
        "print(\"✅ Feature Selection Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSvNr_p6cwkt"
      },
      "source": [
        "Feature scaling\n",
        "lets now scale our selected features to have zero mean and unit variance for neural network training\n",
        "Now, let's prepare our data for the deep learning models. We need to scale our features properly:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale our selected features to have zero mean and unit variance\n",
        "# This is crucial for neural network training\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Convert to PyTorch tensors for deep learning\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test.values)"
      ],
      "metadata": {
        "id": "_Ay7wEf4IvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model shall be the teacher model, we shall build and train it. This will be a deep neural network that learns the complex patterns in our post-harvest loss data:"
      ],
      "metadata": {
        "id": "iPrnZ7zMJPUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TeacherModel, self).__init__()\n",
        "\n",
        "        # Create a deep architecture suitable for complex pattern recognition\n",
        "        self.layers = nn.Sequential(\n",
        "            # First layer - takes our selected features as input\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),  # Batch normalization for stable training\n",
        "            nn.Dropout(0.3),      # Dropout for regularization\n",
        "\n",
        "            # Hidden layers\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "\n",
        "            # Output layer - predicts loss percentages\n",
        "            nn.Linear(64, y_train.shape[1])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize teacher model with our selected feature count\n",
        "teacher_model = TeacherModel(X_train_selected.shape[1])\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Create a training function with proper monitoring\n",
        "def train_teacher(model, epochs=100, batch_size=32):\n",
        "    \"\"\"\n",
        "    Trains the teacher model with batch processing and progress monitoring\n",
        "    \"\"\"\n",
        "    # Convert data to DataLoader for batch processing\n",
        "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    training_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set to training mode\n",
        "        epoch_losses = []\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Forward pass\n",
        "            teacher_optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            teacher_optimizer.step()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        training_losses.append(avg_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_predictions = model(X_test_tensor)\n",
        "                val_loss = criterion(val_predictions, y_test_tensor)\n",
        "                print(f'Validation Loss: {val_loss.item():.4f}')\n",
        "\n",
        "    return training_losses\n",
        "\n",
        "# Train the teacher model\n",
        "print(\"Training Teacher Model...\")\n",
        "teacher_losses = train_teacher(teacher_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUi_6twRJNrc",
        "outputId": "f13ec52c-aa29-41b4-a614-e22b8739e8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Teacher Model...\n",
            "Epoch 0, Average Loss: 0.0227\n",
            "Validation Loss: 0.0012\n",
            "Epoch 10, Average Loss: 0.0013\n",
            "Validation Loss: 0.0010\n",
            "Epoch 20, Average Loss: 0.0013\n",
            "Validation Loss: 0.0010\n",
            "Epoch 30, Average Loss: 0.0013\n",
            "Validation Loss: 0.0010\n",
            "Epoch 40, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n",
            "Epoch 50, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n",
            "Epoch 60, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n",
            "Epoch 70, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n",
            "Epoch 80, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n",
            "Epoch 90, Average Loss: 0.0012\n",
            "Validation Loss: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create our Student Model, which will be simpler but will learn from the Teacher Model through knowledge distillation:"
      ],
      "metadata": {
        "id": "unDlBQObVDek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(StudentModel, self).__init__()\n",
        "\n",
        "        # Simpler architecture that will be enhanced through knowledge distillation\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "\n",
        "            nn.Linear(32, y_train.shape[1])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize the student model\n",
        "student_model = StudentModel(X_train_selected.shape[1])\n",
        "\n",
        "# Create the knowledge distillation loss function\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=3.0, alpha=0.7):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, student_outputs, teacher_outputs, targets):\n",
        "        \"\"\"\n",
        "        Combines knowledge distillation loss with regular supervised loss\n",
        "        \"\"\"\n",
        "        # Soften probability distributions\n",
        "        soft_targets = nn.functional.softmax(teacher_outputs / self.temperature, dim=1)\n",
        "        soft_predictions = nn.functional.log_softmax(student_outputs / self.temperature, dim=1)\n",
        "\n",
        "        # Calculate distillation loss and regular loss\n",
        "        distillation_loss = self.criterion(soft_predictions, soft_targets) * (self.temperature ** 2)\n",
        "        student_loss = self.mse(student_outputs, targets)\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = (self.alpha * distillation_loss) + ((1 - self.alpha) * student_loss)\n",
        "        return total_loss\n",
        "\n",
        "# Set up student training\n",
        "distillation_criterion = DistillationLoss()\n",
        "student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "def train_student_with_distillation(student_model, teacher_model, epochs=100, batch_size=32):\n",
        "    \"\"\"\n",
        "    Trains the student model using knowledge distillation from the teacher\n",
        "    \"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    teacher_model.eval()  # Set teacher to evaluation mode\n",
        "    training_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        student_model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Get teacher predictions\n",
        "            with torch.no_grad():\n",
        "                teacher_predictions = teacher_model(batch_X)\n",
        "\n",
        "            # Train student\n",
        "            student_optimizer.zero_grad()\n",
        "            student_predictions = student_model(batch_X)\n",
        "\n",
        "            # Calculate distillation loss\n",
        "            loss = distillation_criterion(student_predictions, teacher_predictions, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            student_optimizer.step()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        training_losses.append(avg_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return training_losses\n",
        "\n",
        "# Train the student model\n",
        "print(\"\\nTraining Student Model with Knowledge Distillation...\")\n",
        "student_losses = train_student_with_distillation(student_model, teacher_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvJA3XcqU825",
        "outputId": "b30aaee7-7f9e-4e26-f235-cc849f817ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Student Model with Knowledge Distillation...\n",
            "Epoch 0, Average Loss: 0.0191\n",
            "Epoch 10, Average Loss: 0.0004\n",
            "Epoch 20, Average Loss: 0.0004\n",
            "Epoch 30, Average Loss: 0.0004\n",
            "Epoch 40, Average Loss: 0.0004\n",
            "Epoch 50, Average Loss: 0.0004\n",
            "Epoch 60, Average Loss: 0.0004\n",
            "Epoch 70, Average Loss: 0.0004\n",
            "Epoch 80, Average Loss: 0.0004\n",
            "Epoch 90, Average Loss: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation framework"
      ],
      "metadata": {
        "id": "8u7q1xPizQeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=3.0):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_outputs, teacher_outputs, targets):\n",
        "        soft_targets = nn.functional.softmax(teacher_outputs / self.temperature, dim=1)\n",
        "        student_log_softmax = nn.functional.log_softmax(student_outputs / self.temperature, dim=1)\n",
        "        distillation_loss = self.criterion(student_log_softmax, soft_targets)\n",
        "        student_loss = criterion(student_outputs, targets)\n",
        "        return 0.7 * distillation_loss + 0.3 * student_loss\n",
        "\n",
        "distillation_criterion = DistillationLoss()\n",
        "student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "V0miiAH9zOvc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep q learning environment"
      ],
      "metadata": {
        "id": "AKWdR1wOz6Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwJiX_96ogM",
        "outputId": "f7adbd2b-b218-47b1-8035-311ea0c5fd7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from stable_baselines3 import DQN\n",
        "class PostHarvestEnv(gym.Env):\n",
        "    def __init__(self, X, y):\n",
        "        super(PostHarvestEnv, self).__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        self.action_space = spaces.Discrete(10)  # 10 different prediction adjustments\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(X.shape[1],)\n",
        "        )\n",
        "\n",
        "    def step(self, action):\n",
        "        # Implement environment dynamics\n",
        "        prediction = self.student_model(self.X[self.current_step])\n",
        "        adjustment = (action - 5) / 10.0  # Convert action to adjustment\n",
        "        final_prediction = prediction + adjustment\n",
        "\n",
        "        reward = -abs(final_prediction - self.y[self.current_step])\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.X)\n",
        "\n",
        "        return self.X[self.current_step], reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        return self.X[self.current_step]"
      ],
      "metadata": {
        "id": "lgZNPffVz-ni"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning agent"
      ],
      "metadata": {
        "id": "xKl7iPfq1kci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shimmy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBL4nzL7sTt",
        "outputId": "357dd444-7578-491a-b038-d68f73e4b104"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = PostHarvestEnv(X_train_scaled, y_train.values)\n",
        "dqn_agent = DQN(\"MlpPolicy\", env, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HM9H9591nEc",
        "outputId": "5c2751a0-20ee-4f51-acbe-ef1f2cc29540"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensamble model"
      ],
      "metadata": {
        "id": "bu-YmqTf8A6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [model(X) for model in self.models]\n",
        "        return torch.mean(torch.stack(predictions), dim=0)\n",
        "\n",
        "ensemble = EnsembleModel([teacher_model, student_model])\n"
      ],
      "metadata": {
        "id": "_w3UjPNa8AQA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series model\n"
      ],
      "metadata": {
        "id": "rJ4AyMKe8VmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesLSTM(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TimeSeriesLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, 64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, y_train.shape[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return self.fc(lstm_out[:, -1, :])\n",
        "\n",
        "ts_model = TimeSeriesLSTM(X_train_selected.shape[1])"
      ],
      "metadata": {
        "id": "5RsaYdXq8YFY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncertainty-Aware Model"
      ],
      "metadata": {
        "id": "qo6uxrft8aQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(BayesianNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, y_train.shape[1] * 2)  # Mean and variance\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        mean, var = torch.chunk(x, 2, dim=1)\n",
        "        return mean, torch.exp(var)\n",
        "\n",
        "bayesian_model = BayesianNetwork(X_train_selected.shape[1])"
      ],
      "metadata": {
        "id": "82renuV08fcp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning model"
      ],
      "metadata": {
        "id": "ZThTdUUC8urm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.base = base_model\n",
        "        self.adaptation = nn.Linear(y_train.shape[1], y_train.shape[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.adaptation(x)\n",
        "\n",
        "transfer_model = TransferModel(teacher_model)"
      ],
      "metadata": {
        "id": "jC7xBFCQ8yZE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meta-learning model"
      ],
      "metadata": {
        "id": "VzPVxKmf82vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self, model_list):\n",
        "        super(MetaLearner, self).__init__()\n",
        "        self.models = nn.ModuleList(model_list)\n",
        "        self.attention = nn.Linear(len(model_list), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        predictions = [model(x) for model in self.models]\n",
        "        stacked = torch.stack(predictions, dim=1)\n",
        "        weights = torch.softmax(self.attention(stacked), dim=1)\n",
        "        return torch.sum(weights * stacked, dim=1)\n",
        "\n",
        "meta_learner = MetaLearner([\n",
        "    teacher_model, student_model, ts_model, bayesian_model, transfer_model\n",
        "])"
      ],
      "metadata": {
        "id": "UEx8B5LK86Td"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluation pipeline"
      ],
      "metadata": {
        "id": "HwmpP1SP8_Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(models, X_train, y_train, X_test, y_test, epochs=100):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        # Training\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if isinstance(model, BayesianNetwork):\n",
        "                mean, var = model(X_train_tensor)\n",
        "                loss = gaussian_nll_loss(mean, var, y_train_tensor)\n",
        "            else:\n",
        "                outputs = model(X_train_tensor)\n",
        "                loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(model, BayesianNetwork):\n",
        "                predictions, _ = model(X_test_tensor)\n",
        "            else:\n",
        "                predictions = model(X_test_tensor)\n",
        "\n",
        "            mse = mean_squared_error(y_test, predictions.numpy())\n",
        "            r2 = r2_score(y_test, predictions.numpy())\n",
        "\n",
        "            results[name] = {'MSE': mse, 'R2': r2}\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    'Teacher': teacher_model,\n",
        "    'Student': student_model,\n",
        "    'Ensemble': ensemble,\n",
        "    'TimeSeries': ts_model,\n",
        "    'Bayesian': bayesian_model,\n",
        "    'Transfer': transfer_model,\n",
        "    'MetaLearner': meta_learner\n",
        "}\n",
        "\n",
        "results = train_and_evaluate(models, X_train_tensor, y_train_tensor,\n",
        "                           X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Print results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"R2: {metrics['R2']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "eLb6zuEs9FqN",
        "outputId": "0ca62000-4b42-4fe8-9115-3b166d06ac97"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Teacher...\n",
            "Epoch 0, Loss: 0.0012\n",
            "Epoch 10, Loss: 0.0012\n",
            "Epoch 20, Loss: 0.0012\n",
            "Epoch 30, Loss: 0.0012\n",
            "Epoch 40, Loss: 0.0012\n",
            "Epoch 50, Loss: 0.0012\n",
            "Epoch 60, Loss: 0.0012\n",
            "Epoch 70, Loss: 0.0012\n",
            "Epoch 80, Loss: 0.0012\n",
            "Epoch 90, Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mean_squared_error' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4f57cf4a0714>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m }\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m results = train_and_evaluate(models, X_train_tensor, y_train_tensor, \n\u001b[0m\u001b[1;32m     51\u001b[0m                            X_test_tensor, y_test_tensor)\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-4f57cf4a0714>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(models, X_train, y_train, X_test, y_test, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1JPBlLSgHz0y8rgclcaIvk_Aah1_fkJrh",
      "authorship_tag": "ABX9TyPFhRcsNWP+KwWi4p/0DU8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}